{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "import pickle\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'liu_zhi_ping'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def precessname(name):   \n",
    "# #    name = name.strip().lower().replace(' ', '_')\n",
    "#     name = str(name).strip().lower()\n",
    "#     name = re.sub('[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～’]+',' ', name)\n",
    "#     name = re.sub( r'\\s{2,}', ' ', name).strip()\n",
    "#     name = name.replace(' ', '_')\n",
    "#     return name\n",
    "# precessname('liu*]][[]]*zhi.ping.')\n",
    "# # print(''.join('liu_zhiping'.split('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Load and Save Data ################\n",
    "\n",
    "def load_json(rfdir, rfname):\n",
    "    with codecs.open(join(rfdir, rfname), 'r', encoding='utf-8') as rf:\n",
    "        return json.load(rf)\n",
    "\n",
    "\n",
    "def dump_json(obj, wfpath, wfname, indent=None):\n",
    "    with codecs.open(join(wfpath, wfname), 'w', encoding='utf-8') as wf:\n",
    "        json.dump(obj, wf, ensure_ascii=False, indent=indent)\n",
    "\n",
    "def precess_df(data):\n",
    "    data = data.dropna(subset=['name']).copy()\n",
    "    data.drop_duplicates(subset=['name','uid'],keep='first',inplace=True)\n",
    "    return data\n",
    "\n",
    "def precessname(name):   \n",
    "#    name = name.strip().lower().replace(' ', '_')\n",
    "    name = str(name).strip().lower()\n",
    "    name = re.sub('[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～’]+',' ', name)\n",
    "    name = re.sub( r'\\s{2,}', ' ', name).strip()\n",
    "    name = name.replace(' ', '')\n",
    "    return name\n",
    "\n",
    "def reverse_name(name):\n",
    "    name = precessname(name)\n",
    "    name = name.split('_')\n",
    "    reverse_name = '_'.join(name[::-1])\n",
    "    return reverse_name\n",
    "\n",
    "def normname(name):\n",
    "    name = ''.join(name.split('_'))\n",
    "    return name\n",
    "\n",
    "def get_pinyin(name):\n",
    "    name = precessname(pinyin.get(name, format='strip', delimiter=\"_\")).replace('_','')\n",
    "    return name\n",
    "\n",
    "##利用pandas的groupby函数来获取需要的数据集\n",
    "def stand_groupby(dataset):\n",
    "    disamb ={}\n",
    "    for name,group in dataset.groupby(['name']):\n",
    "        if normname(name)  in disamb:\n",
    "            disamb[normname(name)] += list(zip(*[group[c].values.tolist() for c in ['uid', 'org_name']]))\n",
    "        elif normname(reverse_name(name)) not in disamb:\n",
    "            disamb[normname(name)]  = list(zip(*[group[c].values.tolist() for c in ['uid', 'org_name']]))\n",
    "        else:        \n",
    "            disamb[normname(reverse_name(name))]+=list(zip(*[group[c].values.tolist() for c in ['uid', 'org_name']]))\n",
    "    return disamb\n",
    "\n",
    "##利用pandas的groupby函数来获取需要的数据集\n",
    "def stand_groupby_norm(dataset):\n",
    "    disamb ={}\n",
    "    for name,group in dataset.groupby(['name']):\n",
    "        if name not in disamb:\n",
    "            disamb[name] = list(zip(*[group[c].values.tolist() for c in ['uid', 'org_name']]))\n",
    "        else:\n",
    "            disamb[name] += list(zip(*[group[c].values.tolist() for c in ['uid', 'org_name']]))\n",
    "    return disamb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "利用pandas读取数据为csv文件花费时间:  0:01:07.649249\n",
      "Index(['uid', 'name', 'org_name'], dtype='object')\n",
      "本轮消歧涉及到的论文数量：27034623\n",
      "本轮消歧之前的作者数量： 11330581\n",
      "csv转化为同名论文集格式总耗时 :  1:01:00.689944\n"
     ]
    }
   ],
   "source": [
    "def dump_data_to_namepub():\n",
    "    yy = pd.read_csv('data/t_018_uid_name_org_preccess1_10.csv',encoding='UTF-8')\n",
    "    print('利用pandas读取数据为csv文件花费时间: ',datetime.now()-start_time)\n",
    "    \n",
    "    yy['name'] = yy['name'].map(precessname)\n",
    "    print(yy.columns)\n",
    "#     yy['norm_name'] = yy['name'].map(normname)\n",
    "    print('本轮消歧涉及到的论文数量：%d'%(len(yy)))\n",
    "    name_pubs_1_10 = stand_groupby_norm(yy)\n",
    "#     print(name_pubs_50['aaronsonnk'])\n",
    "    del yy\n",
    "    print('本轮消歧之前的作者数量： %d'%(len(name_pubs_1_10)))\n",
    "    dump_json(name_pubs_1_10,'data', 'sci_author_data_1_10_all.json')\n",
    "    print('csv转化为同名论文集格式总耗时 : ',datetime.now()-start_time) \n",
    "dump_data_to_namepub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disamb",
   "language": "python",
   "name": "disamb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
